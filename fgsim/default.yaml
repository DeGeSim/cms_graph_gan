tag: default
loglevel: 20
loglevel_qf: 30
seed: 0
comet_project_name: "jetnet-disctd"
ray: False
remote: False

models:
    gen:
        name: gen_deeptree
        params: ${model_param_options[${models.gen.name}]}
        losses: "${optionlist:${loss_options},${models.gen.losses_list}}"
        losses_list: ${listadd:${gan_mode_options[${training.gan_mode}][gen]},${models.gen.additional_losses_list}}
        additional_losses_list: []
        optim:
            name: Adam
            params: ${optim_options[gen][${models.gen.optim.name}]}
        scheduler:
            name: CyclicLR
            params: ${scheduler_options[${models.gen.scheduler.name}]}
        retain_graph_on_backprop: True
    disc:
        name: disc_mp
        params: ${model_param_options[${models.disc.name}]}
        losses: "${optionlist:${loss_options},${models.disc.losses_list}}"
        losses_list: ${gan_mode_options[${training.gan_mode}][disc]}
        optim:
            name: Adam
            params: ${optim_options[disc][${models.gen.optim.name}]}
        scheduler:
            name: NullScheduler
            params: ${scheduler_options[${models.disc.scheduler.name}]}
        retain_graph_on_backprop: False


optim_options:
    gen:
        Adam:
            weight_decay: 1.0e-4
            lr: 1.0e-05
            betas: [0.9, 0.999]
        SGD:
            lr: 1.0e-05
        RMSprop:
            lr: 1.0e-05
        FakeOptimizer: {}
    disc:
        Adam:
            weight_decay: 1.0e-4
            lr: 3.0e-4
            betas: [0.9, 0.999]
        SGD:
            lr: 2.e-4
        RMSprop:
            lr: 2.e-4
        FakeOptimizer: {}


scheduler_options:
    NullScheduler: {}
    OneCycleLR:
        max_lr_factor: 100
        total_steps: ${prod:500, 575}
    CosineAnnealingWarmRestarts:
        T_0: 10_000
    CyclicLR:
      max_lr_factor: 10
      base_lr: 1.0e-05
      step_size_up: 10000
      cycle_momentum: false


gan_mode_options:
    CE:
        gen: [CEGenLoss]
        disc: [CEDiscLoss]
    W:
        gen: [WGenLoss]
        disc: [WDiscLoss, GradientPenalty]
    MSE:
        gen: [MSEGenLoss]
        disc: [MSEDiscLoss]

ffn:
    activation: LeakyReLU
    hidden_layer_size: 100
    n_layers: 3
    activation_params:
        LeakyReLU:
            negative_slope: 0.1
        ReLU: {}
        SELU: {}
        Tanh: {}
    weight_init_method: default
    norm: batchnorm
    dropout: 0.0

layer_options:
    DeepConv:
        add_self_loops: True
        nns: upd
        msg_nn_include_edge_attr: False
        msg_nn_include_global: False
        msg_nn_final_linear: True
        upd_nn_include_global: True
        upd_nn_final_linear: True
        residual: False
    GINConv:
        final_linear: True
        bias: False
    GINCConv:
        final_linear: True

model_param_options:
    gen_deeptree:
        dim_red_in_branching: True
        n_global: 10
        n_cond: ${len:${loader.y_features}}
        branching_param:
            residual: True
            final_linear: True
            norm: batchnorm
            res_mean: False
            res_final_layer: True
        connect_all_ancestors: True
        ancestor_mpl:
            n_mpl: 1
            n_hidden_nodes: 100
            conv_name: GINConv
            skip_connecton: True
            layer_param: ${layer_options[${model_param_options.gen_deeptree.ancestor_mpl.conv_name}]}
        child_mpl:
            n_mpl: 0
            n_hidden_nodes: 100
            conv_name: GINConv
            skip_connecton: True
            layer_param: ${layer_options[${model_param_options.gen_deeptree.child_mpl.conv_name}]}
        final_layer_scaler: False
    gen_treepc:
        features: [96, 64, 64, 64, 64, 64, 4]
        degrees: [2, 2, 2, 2, 2, 64]
        support: 10
    gen_edgeconv:
        n_layers: 5
        n_features: 2
        n_global: 5
    gen_linear:
        random_size: 64
        n_points: ${loader[n_points]}
        n_features: ${loader[n_features]}
        batch_size: ${loader[batch_size]}
    gen_fake: {}
    gen_moons: {}
    disc_fake: {}
    disc_hlvs: {}
    disc_benno: {}
    disc_deeptree: {}
    disc_diffpool: {}
    disc_dt_tsumt2: {}
    disc_dt_wlevels: {}
    disc_dt_wall: {}
    disc_dt_epictsumt: {}
    disc_gat:
      n_features: ${loader[n_features]}
      n_points: ${loader[n_points]}
      n_cond: ${len:${loader.y_features}}
    disc_clic:
        n_features: ${loader[n_features]}
        n_prop: 20
        n_global: 2
        n_nn: 8
    disc_clicgat: ${model_param_options[disc_clic]}
    disc_graphgym:
        n_features: ${loader[n_features]}
        n_nn: 8
    disc_treepc:
        features: ${tree[features]} # [4, 64, 128, 256]
    disc_pointnet:
        batch_size: ${loader[batch_size]}
        n_points: ${loader[n_points]}
        n_features: ${loader[n_features]}
    disc_pointnet2: ${model_param_options[disc_pointnet]}
    disc_prog:
        leveldisc: disc_graphgym
        levelparams: ${model_param_options[disc_graphgym]}
    disc_pointnetmix:
        pointnetd_fc: [512]
        node_feat_size: 3
        leaky_relu_alpha: 0.2
        pointnetd_pointfc: [64, 128, 1024]
        num_hits: ${loader.n_points}
        mask: False
    disc_pcgan:
        modus: latent
        latent_dim: 128
        z1_dim: 256
        z2_dim: 10
        d_dim: 256
        pool: "max1"
    disc_gapt:
        num_particles: ${loader.n_points}
        input_feat_size: ${loader.n_features}
    gen_gapt:
        num_particles: ${loader.n_points}
        output_feat_size: ${loader.n_features}
        embed_dim: 32
    gen_mp:
        num_particles: ${loader.n_points}
        hidden_node_size: 32
        fe_layers: [96, 160, 192]
        fn_layers: [256, 256]
        fn1_layers: null # end common
        mp_iters: 2
        fe1_layers: null
        final_activation: tanh
        output_node_size: ${loader.n_features}
        input_node_size: 32 # 0 for gen_fc and gen_graphcnn
        lfc: False
        lfc_latent_size: 128
    disc_mp:
        num_particles: ${loader.n_points}
        hidden_node_size: 32
        fe_layers: [25, 25, 25]
        fn_layers: [256, 256]
        fn1_layers: null # end common
        mp_iters: 2
        fe1_layers: null
        final_activation: ""
        dea: True
        dea_sum: True
        fnd: []
        mask_fnd_np: False
        input_node_size: ${loader.n_features}

mpgan_mask:
    mask_feat: False
    mask_feat_bin: False
    mask_weights: False
    mask_manual: False
    mask_exp: False
    mask_real_only: False
    mask_learn: False
    mask_learn_bin: True
    mask_learn_sep: False
    fmg: [64]
    mask_disc_sep: False
    mask_fnd_np: False
    mask_c: True
    mask_fne_np: False

tree: ${mergedefault:${dataset_options},${dataset_name},tree}

loss_options:
    WGenLoss:
        factor: 1.0
    WDiscLoss:
        factor: 1.0
    MSEDiscLoss:
        factor: 1.0
    MSEGenLoss:
        factor: 1.0
    CEDiscLoss:
        factor: 1.0
    CEGenLoss:
        factor: 1.0
    GradientPenalty:
        factor: 1.0
        gamma: 1.0
    mean_dist:
        factor: 1.0
    physics:
        factor: 1.0
    frechetpcdist:
        factor: 1.0
    outside_interval:
        factor: 1.0
        high: 4.0
        low: -4.0
    mmd:
        factor: 1.0
        kernel: "rbf"
        bandwidth: [10, 15, 20, 50]
    mmdpc:
        factor: 1.0
        kernel: "rbf"
        bandwidth: [10, 15, 20, 50]
        batch_wise: False
    fd:
        factor: 0.001
    dcd:
        factor: 1.0
        alpha: 1
        lpnorm: 1
        batch_wise: False
        pow: 2
    cd:
        factor: 1.0
        lpnorm: 1
        batch_wise: False
        pow: 1

training:
    gan_mode: MSE
    disc_steps_per_gen_step: 2
    early_stopping:
        validation_steps: 1000
        improvement: 0.05
    checkpoint_minutes: 15
    smoothing: ${mergedefault:${dataset_options},${dataset_name},smoothing}
    log_interval: 100
    max_epochs: 5000
    val: ${mergedefault:${dataset_options},${dataset_name},validation}

path:
    dataset: "${loader.dataset_path}"
    dataset_processed: "${path.dataset}/pkl_${dataset_name}_${loader_hash}"
    ds_lenghts: "${path.dataset_processed}/filelengths.yaml"
    training: "${path.dataset_processed}/training"
    validation: "${path.dataset_processed}/validation.pt"
    test: "${path.dataset_processed}/test.pt"
    training_glob: "*.pt"
    geo_lup: "data/geo_hgcal/DetIdLUT.root"
    run_path: "wd/${tag}/${hash}"
    tensorboard: "${path.run_path}/tb"
    checkpoint: "${path.run_path}/checkpoint.torch"
    checkpoint_old: "${path.run_path}/checkpoint_old.torch"
    state: "${path.run_path}/state.yaml"
    state_old: "${path.run_path}/state_old.yaml"
    comet_exp_key: "${path.run_path}/comet_experiment_key"

loader: ${mergedefault:${dataset_options},${dataset_name},loader}
dataset_name: default

dataset_options:
    default:
        tree:
            branches: [2, 4, 4,4]
            features:
                - 512
                - 128
                - 64
                - 16
                - 1
        smoothing:
            active: False
        validation:
            interval: "${div:1_000_000,${dataset_options[${dataset_name}][loader][batch_size]}}"
            plot_interval: "${div:1_000_000,${dataset_options[${dataset_name}][loader][batch_size]}}"
        loader:
            rootprefix: "treeMaker/tree"
            dataset_glob: "**/*.root"
            preprocess_training: True
            chunk_size: 1000
            batch_size: 200
            validation_set_size: 10000
            test_set_size: 50000
            scaling_fit_size: 10000
            events_per_file: 10000
            prefetch_batches: 10
            n_workers_transform: 30
            n_workers_stack: 1
            n_workers_postprocess: 1
            n_points: 128
            n_features: ${len:${dataset_options[${dataset_name}][loader][x_features]}}
            x_features: ["foo"]
            x_ftx_energy_pos: 0
            y_features: ["num_particles"]
    jetnet:
        tree:
            branches: [2,3,5]
            features: [64,32,16,3]
        validation:
            interval: 2000
            use_for_stopping: ['w1m','w1p','fpnd']
            metrics: ['w1m','w1p','fpnd','auc',"w1disc"]
        loader:
            dataset_path: "~/fgsim/data/jetnet"
            n_points: 30
            x_features: ["etarel", "phirel", "ptrel"]
            x_ftx_energy_pos: 2
            y_features: ["pt", "eta", "mass", "num_particles"]
            dataset_glob: "**/t.hdf5"
            chunk_size: 5000
            batch_size: 200
    hgcal_graph:
        validation:
            use_for_stopping: ["w1E", "w1x", "w1y", "w1layer"]
            metrics: ["ft_w1","aoc"]
        smoothing:
            active: True
            stds: [0, 0.315, 0.18, .3] # [0, 0.315, 0.2, .15]
            decay: 1e-5
        loader:
            dataset_path: "~/fgsim/data/hgcal_william"
            n_points: 128
            batch_size: 50
            braches:
                id: "simHit_detid"
                energy: "genPh_E"
                hit_energy: "simHit_E"
            x_features: ["E", "x", "y", "layer"]
            y_features: ["E"]
    hgcal_pc: ${dataset_options[hgcal_graph]}
    hgcal_soham:
        loader:
            dataset_path: "~/fgsim/data/soham_pi0"
            n_points: 128
            batch_size: 50
            braches:
                id: "simHit_detid"
                energy: "genPh_E"
                hit_energy: "simHit_E"
            y_features: ["Egen", "eta", "phi"]
            x_features: ["E_hit", "x", "y", "layer"]
    moons:
        loader:
            dataset_path: "~/fgsim/data/moons"
            x_features: ["x", "y"]
            y_features: []
            n_points: 128
            batch_size: 200
            cluster_tree: False
            cluster_method: "random"
